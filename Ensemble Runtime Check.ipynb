{"cells":[{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.session.SparkSession at 0x7f217bf43c90&gt;\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["#Library Imports\nfrom pyspark.sql import SQLContext\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import LinearSVC\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.feature import VectorAssembler \nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.mllib.util import MLUtils\nfrom functools import reduce  \nfrom pyspark.sql import DataFrame\n\nfrom pyspark.sql.types import StructType, StructField, DoubleType\n\n#Schema Used\nfeatureSchema = StructType([StructField('label', DoubleType(), True),\n                     StructField('F1', DoubleType(), True),\n                     StructField('F2', DoubleType(), True),\n                     StructField('F3', DoubleType(), True),                  \n                     StructField('F4', DoubleType(), True),       \n                     StructField('F5', DoubleType(), True),       \n                     StructField('F6', DoubleType(), True),       \n                     StructField('F7', DoubleType(), True),       \n                     StructField('F8', DoubleType(), True),       \n                     StructField('F9', DoubleType(), True),       \n                     StructField('F10', DoubleType(), True),       \n                     StructField('F11', DoubleType(), True),                  \n                     StructField('F12', DoubleType(), True),       \n                     StructField('F13', DoubleType(), True),       \n                     StructField('F14', DoubleType(), True),       \n                     StructField('F15', DoubleType(), True),       \n                     StructField('F16', DoubleType(), True),       \n                     StructField('F17', DoubleType(), True),       \n                     StructField('F18', DoubleType(), True)])   \n\n#Loadind dataset and creating DataFrame\ndf = spark.read.csv('/FileStore/tables/SUSY.csv', header=True, schema=featureSchema)\n\n#Creating feature vector\nfeaturesUsed= ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18']\nassembler = VectorAssembler(inputCols=featuresUsed, outputCol= \"features\")\nassembled = assembler.transform(df)\n\n#Splitting data into Training and Test datset for LSVM\n(trainingData, testData) = assembled.randomSplit([0.7, 0.3])\n\n#Training LSVM\nlsvc = LinearSVC(maxIter=10, regParam=0.1)\npipeline1 = Pipeline(stages=[lsvc])\nmodel1 = pipeline1.fit(trainingData)\n\n#Generating predictions and evaluating model accuracy\npredict1 =model1.transform(testData)\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predict1)\nprint(\"Accuracy = %g\" % (accuracy))\n\n# Seperating TruePredictions and FalsePredictions generated by LSVM\npredict1.createOrReplaceTempView(\"predictVIEW\");\n\nfp = spark.sql(\"select * from predictVIEW where prediction <> label\")\n\ntp = spark.sql(\"select * from predictVIEW where prediction = label\")\n\nfpDF = fp.drop('prediction', 'rawPrediction', 'probability')\n(trainData, TestData) = fpDF.randomSplit([0.7, 0.3])\n\ntpDF = tp.drop('prediction', 'rawPrediction', 'probability')\n(TrainData, TEstData) = tpDF.randomSplit([0.7, 0.3])\n\n#Training and evaluating GBT and DT in Phase 2\ngbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\ndt = DecisionTreeClassifier(labelCol= \"label\", featuresCol= \"features\", maxDepth=5, minInstancesPerNode=20, impurity= \"gini\")\n\npipeline2 = Pipeline(stages=[gbt])\nmodel2 = pipeline2.fit(trainData)\n\npipeline3 = Pipeline(stages=[dt])\nmodel3 = pipeline3.fit(TrainData)\n\npredict2 =model2.transform(testData)\n\npredict3 =model3.transform(TEstData)\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predict2)\nprint(\"Accuracy = %g\" % (accuracy))\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predict3)\nprint(\"Accuracy = %g\" % (accuracy))\n\n#Seperating TruePrections and FalsePredictions generated by GBT and DT\npredict2.createOrReplaceTempView(\"predict2VIEW\");\n\npredict3.createOrReplaceTempView(\"predict3VIEW\");\n\nfp1 = spark.sql(\"select * from predict2VIEW where prediction <> label\")\n\nfp2 = spark.sql(\"select * from predict3VIEW where prediction <> label\")\n\nfp1DF = fp1.drop('prediction', 'rawPrediction', 'probability')\nfp2DF = fp2.drop('prediction', 'rawPrediction', 'probability')\n\n#Creating the combined DataFrame for Phase 3\ndef unionAll(*dfs):\n    return reduce(DataFrame.unionAll, dfs)\n\nfinalDF = unionAll(tpDF, fp1DF, fp2DF)\nfinalDF.count()\n\n#Training and Evalaution of Phase 3\n\n(TData, TstData) = finalDF.randomSplit([0.7, 0.3])\n\nlayers = [18, 19, 20, 2]\ntrainer1 = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n\npipeline4 = Pipeline(stages=[trainer1])\nmodel4 = pipeline4.fit(TData)\n\npredict4 =model4.transform(TstData)\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predict4)\nprint(\"Accuracy = %g\" % (accuracy))\n\npredictionAndLabels = predict4.select(\"prediction\", \"label\")\npredictionAndLabels.rdd.take(2)\n\npredictionAndLabels.rdd.map(tuple).take(2)\nmetrics = BinaryClassificationMetrics(predictionAndLabels.rdd.map(tuple))\n\nprint(\"Area under PR = %s\" % metrics.areaUnderPR)\n\nprint(\"Area under ROC = %s\" % metrics.areaUnderROC)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Accuracy = 0.749009\nAccuracy = 0.328812\nAccuracy = 0.964093\nAccuracy = 0.988979\nArea under PR = 0.969078970404\nArea under ROC = 0.987863296233\n</div>"]}}],"execution_count":2}],"metadata":{"name":"Ensemble Runtime Check","notebookId":2931424989117611},"nbformat":4,"nbformat_minor":0}
